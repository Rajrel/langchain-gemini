{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fe9818-c0a6-4518-ae8e-a6af66fc9d06",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. \n",
    "\n",
    "There are third-party retriever integrations as well :  https://python.langchain.com/v0.1/docs/integrations/retrievers/\n",
    "\n",
    "Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "\n",
    "| Name                     | Index Type                    | Uses an LLM | When to Use                                                                                              | Description                                                                                                                                                          |\n",
    "|--------------------------|-------------------------------|-------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Vectorstore              | Vectorstore                   | No          | If you are just getting started and looking for something quick and easy.                                 | This is the simplest method and the one that is easiest to get started with. It creates embeddings for each piece of text.                                           |\n",
    "| ParentDocument           | Vectorstore + Document Store  | No          | If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. | This indexes multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). |\n",
    "| Multi Vector             | Vectorstore + Document Store  | Sometimes   | If you are able to extract information from documents that you think is more relevant to index than the text itself. | This creates multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions. |\n",
    "| Self Query               | Vectorstore                   | Yes         | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n",
    "| Contextual Compression   | Any                           | Sometimes   | If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM. | This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM. |\n",
    "| Time-Weighted Vectorstore| Vectorstore                   | No          | If you have timestamps associated with your documents, and you want to retrieve the most recent ones      | This fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents).         |\n",
    "| Multi-Query Retriever    | Any                           | Yes         | If users are asking questions that are complex and require multiple pieces of distinct information to respond | This uses an LLM to generate multiple queries from the original one. This is useful when the original query needs pieces of information about multiple topics to be properly answered. By generating multiple queries, we can then fetch documents for each of them. |\n",
    "| Ensemble                 | Any                           | No          | If you have multiple retrieval methods and want to try combining them.                                    | This fetches documents from multiple retrievers and then combines them.                                                                                                |\n",
    "| Long-Context Reorder     | Any                           | No          | If you are working with a long-context model and noticing that it's not paying attention to information in the middle of retrieved documents. | This fetches documents from an underlying retriever, and then reorders them so that the most similar are near the beginning and end. This is useful because it's been shown that for longer context models they sometimes don't pay attention to information in the middle of the context window. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33a148d-bccf-4840-a523-624aa3ac0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd \n",
    "import os\n",
    "load_dotenv()\n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d970433-436b-4d0d-abfc-6d7da631cf2e",
   "metadata": {},
   "source": [
    "##### Vector store-backed retriever\n",
    "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6ee79e-bcb9-448e-8826-684634a3a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Load the document,  \n",
    "raw_documents = TextLoader('Data/state_of_the_union.txt', encoding = 'utf-8').load()\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "# embed each chunk\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# load it into the vector store.\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824ef0bd-64d6-42c7-9831-d04466ce5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456ce43b-641e-4b5d-ba95-460b962d8cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'Data/state_of_the_union.txt'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"what did he say about ketanji brown jackson\")\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7309c4a0-1b65-47e1-8444-b3187e490558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'Data/state_of_the_union.txt'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can specify search type\n",
    "#retriever = db.as_retriever(search_type=\"mmr\")\n",
    "#docs = retriever.invoke(\"what did he say about ketanji brown jackson\")\n",
    "\n",
    "\n",
    "# We can also set a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold.\n",
    "# We can also specify search kwargs like k to use when doing retrieval.\n",
    "threshold_retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.3, \"k\":1}\n",
    ")\n",
    "threshold = threshold_retriever.invoke(\"what did he say about ketanji brown jackson\")\n",
    "threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffa687-1703-4da1-8555-305a4d2b442a",
   "metadata": {},
   "source": [
    "##### Langchain-bakced Retrievers\n",
    "```python \n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "\n",
    "When splitting documents for retrieval, there are often conflicting desires:\n",
    "1. You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
    "2. You want to have long enough documents that the context of each chunk is retained.\n",
    "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd7ccc51-2dd9-429f-aff5-f50bf23ed546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90ba16ec-ed56-4965-8ec4-fe5e09e89cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"Data/paul_graham_essay.txt\", encoding = 'utf-8'),\n",
    "    TextLoader(\"Data/state_of_the_union.txt\", encoding = 'utf-8'),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71ae6cd9-43ec-4c9d-ae7e-28223f1de2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=embeddings\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df3dc3fb-0cb5-4137-8031-4a92e81eb318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4d2f2835-b075-4e74-8420-1489f60c1cdf',\n",
       " '0feaba00-5f2a-4d9f-847a-bad7b875a44d']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs, ids=None)\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0c163c3-685a-4ff4-a202-b4ae9b43ca86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n"
     ]
    }
   ],
   "source": [
    "#Let's now call the vector store search functionality - we should see that it returns small chunks (since we're storing the small chunks).\n",
    "\n",
    "sub_docs = vectorstore.similarity_search(\"justice breyer\")\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58c5f7f1-b795-4a16-bed7-182f3bd58b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38539"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's now retrieve from the overall retriever.\n",
    "retrieved_docs = retriever.invoke(\"justice breyer\")\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3e965-ee9f-48c1-9a13-7e9fa0d5fa75",
   "metadata": {},
   "source": [
    "```python \n",
    "    from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "The methods to create multiple vectors per document include:\n",
    "\n",
    "Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).\n",
    "Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
    "Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\n",
    "Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad0c1e-4ec0-4a58-8582-c6e5fd303580",
   "metadata": {},
   "source": [
    "```from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92540c66-9cf8-4676-ad65-86847ce66e2a",
   "metadata": {},
   "source": [
    "##### SelfQueryRetriever\n",
    "```python \n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fc3f79c-cd44-44a2-a9b4-bc90559786b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install lark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e505f26-1d95-4058-86d1-c7bc1ac5f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3238c5-76ff-422e-ac9c-d3411f7bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\", temperature = 0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6bb772b-a952-4694-a49d-86bf288e35f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}),\n",
       " Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example only specifies a filter\n",
    "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb33ad43-0ef2-49a7-8e6b-3e86b314baa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example specifies a query and a filter\n",
    "retriever.invoke(\"Has Greta Gerwig directed any movies about women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de30bdd0-a2ae-48af-aa29-c459cbc6db49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example specifies a query and composite filter\n",
    "retriever.invoke(\n",
    "    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa080778-27f3-4973-a982-17670cd39ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}),\n",
       " Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use the self query retriever to specify  by passing enable_limit=True to the constructor.\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,\n",
    ")\n",
    "\n",
    "# This example only specifies a relevant query\n",
    "retriever.invoke(\"What are two movies about dinosaurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38891c40-3fdc-4eb7-ba5d-c6a29af107e2",
   "metadata": {},
   "source": [
    "###### Contextual compression\n",
    "\n",
    "```python\n",
    "            from langchain.retrievers import ContextualCompressionRetriever\n",
    "            from langchain.retrievers.document_compressors import LLMChainExtractor, LLMChainFilter, EmbeddingsFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d07d15ba-fb91-4e13-8112-99a02129a629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'Data/state_of_the_union.txt'}),\n",
       " Document(page_content='So let’s not abandon our streets. Or choose between safety and equal justice. \\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable. \\n\\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.  \\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.', metadata={'source': 'Data/state_of_the_union.txt'}),\n",
       " Document(page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic.', metadata={'source': 'Data/state_of_the_union.txt'}),\n",
       " Document(page_content='We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI’ve worked on these issues a long time. \\n\\nI know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.', metadata={'source': 'Data/state_of_the_union.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "documents = TextLoader(\"Data/state_of_the_union.txt\", encoding ='utf-8').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "retriever = FAISS.from_documents(texts, embeddings).as_retriever()\n",
    "\n",
    "docs = retriever.invoke(\"What did the president say about Ketanji Brown Jackson\")\n",
    "display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fedb80-0c64-4bc4-bf77-bdd8f7e2a3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'Data/state_of_the_union.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Adding contextual compression with an LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "display(compressed_docs)\n",
    "## LLMChainExtractor iterates over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09a199f9-2e66-4622-b768-b42897248c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'Data/state_of_the_union.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "display(compressed_docs)\n",
    "# The LLMChainFilter is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents\n",
    "#to filter out and which ones to return, without manipulating the document contents.\n",
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce41f97e-3689-4545-acdd-ff2ea062c000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.5)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "display(compressed_docs[0].page_content)\n",
    "# Making an extra LLM call over each retrieved document is expensive and slow. \n",
    "#The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query \n",
    "#and only returning those documents which have sufficiently similar embeddings to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9726585c-4603-44de-9c46-ba4329cb8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stringing compressors and document transformers together\n",
    "\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.5)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0cbdbe7-5884-4621-9317-eaebedf00639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What did the president say about Ketanji Jackson Brown\"\n",
    ")\n",
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4c995-00c9-4dae-872a-16e4f3d9e2b4",
   "metadata": {},
   "source": [
    "##### MultiQueryRetriever\n",
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
    "\n",
    "The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "193be8ac-b05b-443e-b586-883f2a02273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94ac02e7-4542-41a7-8842-983c29bd0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a99e88e1-5da6-4b10-95cf-a130f62a0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e3daf51-eff2-46a8-8787-a7ce1c8b5fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['## Alternative Questions for \"What are the approaches to Task Decomposition?\":', '', '1. **Focusing on the goal:**  What are the different methods for breaking down complex tasks into smaller, manageable subtasks?', '2. **Highlighting the benefits:** How can task decomposition be used to improve efficiency and effectiveness in project management?', '3. **Expanding the scope:** What are the different strategies for task decomposition, including top-down, bottom-up, and hybrid approaches?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0b135f9-090d-4708-9466-e66e07b5e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self-prompting\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# Other inputs\n",
    "question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24db6453-7b2c-43d5-b379-496b80fcb0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: [\"Here are five different versions of the original question, aiming to capture different aspects of the user's intent:\", '', '1. **Focus on definition:** What is regression, according to the course materials?', '2. **Focus on application:** How is regression used in the context of this course?', '3. **Focus on specific type:** What does the course say about linear regression?', '4. **Focus on comparison:** How does the course compare regression to other statistical methods?', '5. **Focus on practical example:** Can you provide an example of how regression is applied in the course material?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.invoke(\"What does the course say about regression?\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f72e7-0b03-4d71-bd71-ad5e3d318fb6",
   "metadata": {},
   "source": [
    "##### Ensemble Retriever\n",
    "The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the Reciprocal Rank Fusion algorithm.\n",
    "\n",
    "By leveraging the strengths of different algorithms, the EnsembleRetriever can achieve better performance than any single algorithm.\n",
    "\n",
    "The most common pattern is to <b>combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity) </b>, because their strengths are complementary. It is also known as \"hybrid search\". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b793a8c-a3c4-4a8e-97f1-8d854ba888bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28cd36e7-4aff-4a2b-b63f-f914bc66e1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install rank_bm25 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7203710-c604-48e7-955b-6b0d21eeae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_1 = [\n",
    "    \"I like apples\",\n",
    "    \"I like oranges\",\n",
    "    \"Apples and oranges are fruits\",\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    ")\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "doc_list_2 = [\n",
    "    \"You like apples\",\n",
    "    \"You like oranges\",\n",
    "]\n",
    "\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list_2, embeddings, metadatas=[{\"source\": 2}] * len(doc_list_2)\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ab3f8a1-3032-46df-aa67-7c263600991e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='I like apples', metadata={'source': 1}),\n",
       " Document(page_content='You like apples', metadata={'source': 2}),\n",
       " Document(page_content='Apples and oranges are fruits', metadata={'source': 1}),\n",
       " Document(page_content='You like oranges', metadata={'source': 2})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ensemble_retriever.invoke(\"apples\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d78a5-1567-4d4b-bdc0-91aa0874fc93",
   "metadata": {},
   "source": [
    "##### Long-Context Reorder\n",
    "No matter the architecture of your model, there is a substantial performance degradation when you include 10+ retrieved documents. In brief: When models must access relevant information in the middle of long contexts, they tend to ignore the provided documents. See: https://arxiv.org/abs/2307.03172\n",
    "\n",
    "To avoid this issue you can re-order documents after retrieval to avoid performance degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5eb04163-2541-4a2c-87ca-a108ce431d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Basquetball is a great sport.\",\n",
    "    \"Fly me to the moon is one of my favourite songs.\",\n",
    "    \"The Celtics are my favourite team.\",\n",
    "    \"This is a document about the Boston Celtics\",\n",
    "    \"I simply love going to the movies\",\n",
    "    \"The Boston Celtics won the game by 20 points\",\n",
    "    \"This is just a random text.\",\n",
    "    \"Elden Ring is one of the best games in the last 15 years.\",\n",
    "    \"L. Kornet is one of the best Celtics players.\",\n",
    "    \"Larry Bird was an iconic NBA player.\",\n",
    "]\n",
    "\n",
    "# Create a retriever\n",
    "texts_retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10})\n",
    "longcontextquery = \"What can you tell me about the Celtics?\"\n",
    "\n",
    "# Get relevant documents ordered by relevance score\n",
    "longcontext = texts_retriever.invoke(longcontextquery)\n",
    "longcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9442c04d-da3b-426c-bc37-e553e68846d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics'),\n",
       " Document(page_content='This is a document about the Boston Celtics')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# Reorder the documents:\n",
    "# Less relevant document will be at the middle of the list and more\n",
    "# relevant elements at beginning / end.\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(longcontext)\n",
    "\n",
    "# Confirm that the 4 relevant documents are at beginning and end.\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "036bf45f-3f08-46c3-8198-b2d0319e1c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, you can only tell that the text is about the Boston Celtics. There is no information about the team itself, such as their history, players, or accomplishments. \n",
      "\n",
      "To learn more about the Celtics, you would need to consult other sources like:\n",
      "\n",
      "* **Wikipedia:** A great starting point for general information about the team.\n",
      "* **Official team website:** Provides news, schedules, and player information.\n",
      "* **Sports news websites:** Offer articles, analysis, and commentary on the Celtics.\n",
      "* **Books and documentaries:** Provide in-depth coverage of the team's history and legacy. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given these texts:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Please answer the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")\n",
    "\n",
    "# Create and invoke the chain:\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "response = chain.invoke({\"context\": reordered_docs, \"query\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831606d-dbf6-4111-8c76-1269c5c15d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
