{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8aedbf-cabd-4953-8dfd-db66d25d8536",
   "metadata": {},
   "source": [
    "## Message Trimming\n",
    "All models have finite context windows, meaning there's a limit to how many tokens they can take as input. If you have very long messages or a chain/agent that accumulates a long message is history, you'll need to manage the length of the messages you're passing in to the model.\n",
    "\n",
    "The trim_messages util provides some basic strategies for trimming a list of messages to be of a certain token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc3501d-a314-4e36-98cb-3432e55e934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable pip version check\n",
    "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2405bc47-7f57-43e1-8524-304217e3f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898f28bf-c7e6-4ac3-b8fb-4429e135cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import  ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0) # \"chat-bison@001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8eff8b-f4dc-467b-bb0d-b91714c3e966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='and who is harrison chasing anyways'),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the last max_tokens tokens\n",
    "\"\"\" To get the last max_tokens in the list of Messages we can set strategy=\"last\"\"\"\"\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"what do you call a speechless parrot\"),\n",
    "]\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3704f8b7-e02f-4d36-93d7-186d1eb469a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to always keep the initial system message we can specify include_system=True:\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b94cb1-3f20-405e-9a45-f7032c913811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to allow splitting up the contents of a message we can specify allow_partial=True:\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=56,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aae6d91-add1-4501-a648-5c7789e570ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
       " HumanMessage(content='and who is harrison chasing anyways'),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
       " HumanMessage(content='what do you call a speechless parrot')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we need to make sure that our first message (excluding the system message) is always of a specific type, we can specify start_on:\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=60,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b89ffd7-3b97-4413-a359-bec0cd75bf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A Polly want a cracker? ðŸ˜‰ \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e9b250cd-e9fb-4668-b198-de104436e549-0', usage_metadata={'input_tokens': 22, 'output_tokens': 7, 'total_tokens': 29})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chaining\n",
    "\"\"\"trim_messages can be used in an imperatively (like above) or declaratively,\n",
    "making it easy to compose with other components in a chain\"\"\"\n",
    "\n",
    "# Notice we don't pass in messages. This creates\n",
    "# a RunnableLambda that takes messages as input\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | model\n",
    "chain.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c22e6-a175-4a0d-80db-5e237f2191a0",
   "metadata": {},
   "source": [
    "#### Using with ChatMessageHistory\n",
    "Trimming messages is especially useful when working with chat histories, which can get arbitrarily long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58eca518-8401-4cca-933c-7e3f9d287d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A Polly want a cracker? ðŸ˜‰ \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-d9a4bfa6-64b5-460f-a2d4-5580d6739884-0', usage_metadata={'input_tokens': 22, 'output_tokens': 7, 'total_tokens': 29})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_history = InMemoryChatMessageHistory(messages=messages[:-1])\n",
    "\n",
    "\n",
    "def dummy_get_session_history(session_id):\n",
    "    if session_id != \"1\":\n",
    "        return InMemoryChatMessageHistory()\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | model\n",
    "chain_with_history = RunnableWithMessageHistory(chain, dummy_get_session_history)\n",
    "chain_with_history.invoke(\n",
    "    [HumanMessage(\"what do you call a speechless parrot\")],\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25dad1-5c20-4777-837b-4a7fb78d02c1",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "550399bc-8a02-46b7-8d4b-bc16bfb0531b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='example output', name='example_assistant', id='3'),\n",
       " AIMessage(content='real output', name='alice', id='5')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "\n",
    "filter_messages(messages, include_types=\"ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d614b90-0e1b-4ae4-a337-709ba30ca61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', id='1'),\n",
       " HumanMessage(content='real input', name='bob', id='4'),\n",
       " AIMessage(content='real output', name='alice', id='5')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "981e28c1-b009-457b-879c-eddd9253dda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', name='example_user', id='2'),\n",
       " HumanMessage(content='real input', name='bob', id='4'),\n",
       " AIMessage(content='real output', name='alice', id='5')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad8d4e81-5ad5-4aa5-9193-cd84c6cd41c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Please provide me with some context or information so I can understand what you're asking for.  For example, you could tell me:\\n\\n* **What kind of input are you looking for?** (e.g., a question, a task, a story idea)\\n* **What is the topic or subject of the input?** (e.g., technology, history, cooking)\\n* **What is the desired output?** (e.g., a response, a solution, a creative piece)\\n\\nThe more information you give me, the better I can assist you. ðŸ˜Š \\n\", response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-55746287-15a3-41f9-ad17-1b2b4af21219-0', usage_metadata={'input_tokens': 11, 'output_tokens': 122, 'total_tokens': 133})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Chaining \n",
    "\n",
    "filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "chain = filter_ | llm\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2838576c-4145-4015-b462-3bd3ea6d189a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', id='1'),\n",
       " HumanMessage(content='real input', name='bob', id='4'),\n",
       " AIMessage(content='real output', name='alice', id='5')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268688b0-f934-43ef-9f6e-5c2c9acc33cd",
   "metadata": {},
   "source": [
    "#### Merge consecutive messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "605d2062-c035-4967-959c-54b01eeb5a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\")\n",
      "\n",
      "HumanMessage(content=[{'type': 'text', 'text': \"i wonder why it's called langchain\"}, 'and who is harrison chasing anyways'])\n",
      "\n",
      "AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    merge_message_runs,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    SystemMessage(\"you always respond with a joke.\"),\n",
    "    HumanMessage([{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    AIMessage(\"Why, he's probably chasing after the last cup of coffee in the office!\"),\n",
    "]\n",
    "\n",
    "merged = merge_message_runs(messages)\n",
    "print(\"\\n\\n\".join([repr(x) for x in merged]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04e3f219-4dac-4d48-8ad8-00cfbf5e39b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"...But seriously, LangChain is a framework for building applications that use large language models (LLMs) like me. It's named after the chain of thought that LLMs use to process information. And as for Harrison, I'm not sure who you're referring to. Could you give me more context? \\n\", response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-9e1c8cf1-8d3e-4bea-afae-2d681535459c-0', usage_metadata={'input_tokens': 77, 'output_tokens': 65, 'total_tokens': 142})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chaining \n",
    "\n",
    "merger = merge_message_runs()\n",
    "chain = merger | model\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fc7c8-c9d3-41ec-8924-8d36874df36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
