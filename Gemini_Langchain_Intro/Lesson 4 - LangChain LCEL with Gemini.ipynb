{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fd9219-5520-452b-8665-d89a0a53eeb0",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)\n",
    "LangChain Expression Language, or LCEL, is a declarative way to chain LangChain components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554e5988-1f7d-4018-880b-d7286b930708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable pip version check\n",
    "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83807d8f-95b4-4443-90b0-b0765eb6de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872e6b2e-99b6-490a-90f2-5b37da807a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import  ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\") # \"chat-bison@001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a6c2d-05b3-4af7-90ba-244e75af9b12",
   "metadata": {},
   "source": [
    "#### Chaining Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ead8e51-8670-47ec-9ca1-e01e402630fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the married man get a vasectomy? \\n\\nHe wanted to make sure his wife wouldn't be able to blame him for anything anymore! \\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Using The pipe operator: |\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke ( {\"topic\": \"Marriage\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c5e379-009e-4592-aa59-c55f56ac5380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That\\'s a pretty good one! It\\'s a classic dad joke - simple, silly, and relies on a pun.  It\\'s funny because it plays on the word \"high\" and the unexpected image of someone using a ladder to reach the high tide. \\n\\nI\\'d love to hear another joke! üòÑ \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Using Coercion\n",
    "\"\"\"We can even combine this chain with more runnables to create another chain. \n",
    "This may involve some input/output formatting using other types of runnables, \n",
    "depending on the required inputs and outputs of the chain components\"\"\"\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "composed_chain.invoke({\"topic\": \"Marriage\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71baac2a-8c56-4ec8-a407-50308fab6557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, this is a funny joke! Here\\'s why:\\n\\n* **Wordplay:** The joke uses the phrase \"You\\'re not listening!\" in a clever way. It plays on the common complaint of one partner feeling unheard, but then twists it by having the husband realize his wife was actually right all along.\\n* **Unexpected Twist:** The joke sets up the expectation that the husband is annoyed by his wife\\'s complaint. The punchline then subverts this expectation, creating a humorous surprise.\\n* **Relatability:**  Many people can relate to the experience of feeling unheard in a relationship, making the joke resonate with a wider audience.\\n\\nOverall, the joke is funny because it\\'s clever, unexpected, and relatable.  The wink face at the end adds to the playful tone. \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using the Pipe Method\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "composed_chain_with_pipe = (\n",
    "    RunnableParallel({\"joke\": chain})\n",
    "    .pipe(analysis_prompt)\n",
    "    .pipe(model)\n",
    "    .pipe(StrOutputParser())\n",
    ")\n",
    "\n",
    "composed_chain_with_pipe.invoke({\"topic\": \"Marriage\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fd94d-c1db-480a-8f4c-2bf3f026abab",
   "metadata": {},
   "source": [
    "#### Streaming runnables\n",
    "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
    "Important LangChain primitives like chat models, output parsers, prompts, retrievers, and agents implement the LangChain <b> Runnable Interface </b>.\n",
    "This interface provides two general approaches to stream content:\n",
    "\n",
    "<b> sync stream and async astream </b> :  a default implementation of streaming that streams the final output from the chain.\n",
    "\n",
    "<b> async astream_events and async astream_log</b>: these provide a way to stream both intermediate steps and final output from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c213dfb-b76a-4072-8d68-8ba952702df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| sea is not a single color, but rather appears in a range of shades depending| on various factors:\n",
      "\n",
      "* **Depth:**  Shallow water appears turquoise or green| due to sunlight reflecting off the seafloor and suspended particles. Deeper water appears blue because red and yellow wavelengths of light are absorbed by water molecules, leaving the blue| wavelengths to be reflected back.\n",
      "* **Sunlight:** The angle of the sun affects the color of the sea. At sunrise and sunset, the light travels through| more atmosphere, scattering the blue wavelengths and leaving reds and oranges to be reflected.\n",
      "* **Turbidity:** The presence of sediment, algae, and other particles can make the water appear green, brown, or even red.\n",
      "* **|Sky:** The color of the sky can also affect the perception of the sea's color. A clear blue sky will make the sea appear bluer, while a cloudy sky will make it appear grayer.\n",
      "\n",
      "**So, the \"|color\" of the sea is actually a complex interplay of these factors, and can vary greatly depending on the specific location and time of day.**\n",
      "|"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sea?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e816ffb7-aa9f-4ef7-a8a5-cead50ae88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| sky is **blue** during the day due to a phenomenon called **Rayleigh| scattering**. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Sunlight:** Sunlight is| made up of all the colors of the rainbow.\n",
      "* **Scattering:** When sunlight enters the Earth's atmosphere, it collides with gas molecules.|\n",
      "* **Wavelengths:** Blue light has a shorter wavelength than other colors, so it gets scattered more easily by the molecules in the air.\n",
      "* **|Our Eyes:** Our eyes are most sensitive to blue light, so we see the scattered blue light as the color of the sky.\n",
      "\n",
      "However, the sky can appear other colors at different times of day:\n",
      "\n",
      "* **Sunrise and Sunset:**| The sky appears red and orange because the sunlight has to travel through more atmosphere at these times, scattering away most of the blue light.\n",
      "* **Clouds:** Clouds can appear white or gray because they reflect sunlight.\n",
      "* **Night:**| The sky appears black at night because there is no sunlight to scatter. \n",
      "|"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787f27d-3c56-4412-8742-f5830683c775",
   "metadata": {},
   "source": [
    "#### InputStream\n",
    "What if you wanted to stream JSON from the output as it was being generated? If you were to rely on json.loads to parse the partial json, the parsing would fail as the partial json wouldn't be valid json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db9f91a-3c57-4513-a6b0-bbbeeefd3602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'countries': [{}]}\n",
      "{'countries': [{'name': 'France', 'population': 67.39}]}\n",
      "{'countries': [{'name': 'France', 'population': 67.39}, {'name': 'Spain', 'population': 47.35}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67.39}, {'name': 'Spain', 'population': 47.35}, {'name': 'Japan', 'population': 125.8}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e80f7cb-a2f3-45ad-8bce-db30f5fd0593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    ")\n",
    "\n",
    "\n",
    "# A function that operates on finalized inputs\n",
    "# rather than on an input_stream\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a0d513d-2846-4f6b-8814-af5bea7fb39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names_streaming(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dadfd6-c5a5-4e60-8e55-4f54e5bff056",
   "metadata": {},
   "source": [
    "#### Parallelizing Runnables \n",
    "RunnableParallels are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0d29c6f-ceb5-4ba3-a482-dc4daee9722c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho. \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# The prompt expects input with keys for \"context\" and \"question\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b79fb5c-3f8a-4974-9496-ab782070911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bear say no to dessert? \n",
      "\n",
      "Because he was stuffed! üêª \n",
      "\n",
      "A furry giant, strong and bold,\n",
      "In the forest, stories unfold. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Parallellizing Steps\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "import textwrap\n",
    "\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "result = map_chain.invoke({\"topic\": \"bear\"})\n",
    "\n",
    "joke = result['joke']\n",
    "poem = result['poem']\n",
    "print(joke.content)\n",
    "print(poem.content) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fd327-6c15-4532-b77a-8e0b969e2512",
   "metadata": {},
   "source": [
    "#### @chain decorator\n",
    "We can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52523769-8ce4-4a47-81d1-5be1f162951d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The subject of the joke is **poker**. \\n\\nThe joke uses a pun, playing on the word \"bear\" and its homophone \"bare\" to create a humorous situation about why poker wouldn\\'t be a good game to play in the forest. \\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
    "\n",
    "\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    output1 = model.invoke(prompt_val1)\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "    chain2 = prompt2 | model | StrOutputParser()\n",
    "    return chain2.invoke({\"joke\": parsed_output1})\n",
    "\n",
    "\n",
    "custom_chain.invoke(\"bears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791417-381d-4052-a29d-5bd7a776b644",
   "metadata": {},
   "source": [
    "#### Automatic coercion in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8ac08e5-3ed0-452c-abe5-8383d54d2f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The s\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n",
    "\n",
    "\n",
    "### Delimiting the length of the content\n",
    "chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])\n",
    "\n",
    "result = chain_with_coerced_function.invoke({\"topic\": \"bears\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda29bc-4cc1-4b60-bd80-b1b691e3e6c0",
   "metadata": {},
   "source": [
    "#### How to inspect runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abc5af-96ab-49fd-973a-9dcca7bcfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43a61f3c-0ada-4d19-b942-7461592a47c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'40844a25c1b1464895b050582948c09a': Node(id='40844a25c1b1464895b050582948c09a', data=<class 'pydantic.v1.main.RunnableParallel<context,question>Input'>), '06aec70321e242e5a83b0287991832e4': Node(id='06aec70321e242e5a83b0287991832e4', data=<class 'pydantic.v1.main.RunnableParallel<context,question>Output'>), '793a9605c7bf4bd48f2480f1542c50a5': Node(id='793a9605c7bf4bd48f2480f1542c50a5', data=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020C10D08E10>)), 'fe605b09c1b14c1b8c745464391cb2fe': Node(id='fe605b09c1b14c1b8c745464391cb2fe', data=RunnablePassthrough()), '452458793be042958e554c4dc3eb61ee': Node(id='452458793be042958e554c4dc3eb61ee', data=ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])), '133372e4866f4c049b7926d552296a65': Node(id='133372e4866f4c049b7926d552296a65', data=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000020C02394450>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x0000020C02A60310>, default_metadata=())), '7f152083bcbd486db4ee0c51a4e2a332': Node(id='7f152083bcbd486db4ee0c51a4e2a332', data=StrOutputParser()), '7e78238d78304f138187c4f2d5a65826': Node(id='7e78238d78304f138187c4f2d5a65826', data=<class 'pydantic.v1.main.StrOutputParserOutput'>)}, edges=[Edge(source='40844a25c1b1464895b050582948c09a', target='793a9605c7bf4bd48f2480f1542c50a5', data=None, conditional=False), Edge(source='793a9605c7bf4bd48f2480f1542c50a5', target='06aec70321e242e5a83b0287991832e4', data=None, conditional=False), Edge(source='40844a25c1b1464895b050582948c09a', target='fe605b09c1b14c1b8c745464391cb2fe', data=None, conditional=False), Edge(source='fe605b09c1b14c1b8c745464391cb2fe', target='06aec70321e242e5a83b0287991832e4', data=None, conditional=False), Edge(source='06aec70321e242e5a83b0287991832e4', target='452458793be042958e554c4dc3eb61ee', data=None, conditional=False), Edge(source='452458793be042958e554c4dc3eb61ee', target='133372e4866f4c049b7926d552296a65', data=None, conditional=False), Edge(source='7f152083bcbd486db4ee0c51a4e2a332', target='7e78238d78304f138187c4f2d5a65826', data=None, conditional=False), Edge(source='133372e4866f4c049b7926d552296a65', target='7f152083bcbd486db4ee0c51a4e2a332', data=None, conditional=False)])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "420d2572-f597-4cd3-ac3a-4253e5aa805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            +---------------------------------+         \n",
      "            | Parallel<context,question>Input |         \n",
      "            +---------------------------------+         \n",
      "                    **               ***                \n",
      "                 ***                    ***             \n",
      "               **                          **           \n",
      "+----------------------+               +-------------+  \n",
      "| VectorStoreRetriever |               | Passthrough |  \n",
      "+----------------------+               +-------------+  \n",
      "                    **               ***                \n",
      "                      ***         ***                   \n",
      "                         **     **                      \n",
      "           +----------------------------------+         \n",
      "           | Parallel<context,question>Output |         \n",
      "           +----------------------------------+         \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                  +--------------------+                \n",
      "                  | ChatPromptTemplate |                \n",
      "                  +--------------------+                \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                +------------------------+              \n",
      "                | ChatGoogleGenerativeAI |              \n",
      "                +------------------------+              \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                    +-----------------+                 \n",
      "                    | StrOutputParser |                 \n",
      "                    +-----------------+                 \n",
      "                             *                          \n",
      "                             *                          \n",
      "                             *                          \n",
      "                 +-----------------------+              \n",
      "                 | StrOutputParserOutput |              \n",
      "                 +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "retrieval_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d27d5826-1a81-4fd4-8f60-8b80e1ebbcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba5763-3d14-4339-80c2-2429885b3cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
