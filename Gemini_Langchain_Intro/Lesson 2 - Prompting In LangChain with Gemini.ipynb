{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e17061-11be-4bf7-b411-1efb930b56e7",
   "metadata": {},
   "source": [
    "## LangChain Prompts & Example Selectors\n",
    "- Prompts refer to the text that is sent to the language model for processing.\n",
    "- They serve as instructions or queries that elicit specific responses from the model.\n",
    "- Prompts can be simple or more instructional, depending on the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3916426a-85fd-426d-b730-31788aab9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd \n",
    "import os\n",
    "load_dotenv()\n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bdf26-cef3-4637-8edd-2bc63d96a14d",
   "metadata": {},
   "source": [
    "#### String PromptTemplates\n",
    "These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cb8285-d9a7-4c88-9119-a78720186ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you call a cat that always gets what it wants?\n",
      "\n",
      "A purr-suasive cat!\n"
     ]
    }
   ],
   "source": [
    "# Import Prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\")\n",
    "result = llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa569b-b9fb-4c7c-8b64-9dcbb84ff0dd",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplates\n",
    "These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038347d5-9847-4298-8054-ca65fe22f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't cats play poker? \n",
      "\n",
      "Because they always have an ace up their sleeve! ðŸ˜¹ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "from langchain_google_genai.chat_models import  ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0.1) # \"chat-bison@001\"\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e00d4-7839-45fe-9faf-b2d2f5ceadf3",
   "metadata": {},
   "source": [
    "Setting convert_system_message_to_human to True is deprecated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db4df84-bc39-4c01-b3a8-86704ef8b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "result = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"Answer only yes or no.\"),\n",
    "        HumanMessage(content=\"Is apple a fruit?\"),\n",
    "    ]\n",
    ")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacb341-5b7c-4569-84ef-cf3bafb96bec",
   "metadata": {},
   "source": [
    "### MessagesPlaceholder\n",
    "This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4eb5b76-8318-4b45-99ac-441e74a295bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! ðŸ‘‹  How can I help you today? ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "template = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "result = llm.invoke(template)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b452ee-1245-4a86-86c2-6301a4a9d111",
   "metadata": {},
   "source": [
    "### Few Shot Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33411b5-6caf-40a2-aec3-a464e7f6c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of craigslist born?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235171ac-76a5-4784-be90-44e26df187ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# prompt =  prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n",
    "\n",
    "prompt =  prompt.invoke({\"input\": \"Which mountain is higher - Mt Everest or Sandia?\"}).to_string()\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe108d-22fc-447f-a659-e211c6d42730",
   "metadata": {},
   "source": [
    "### Example selectors\n",
    "In case of a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.\n",
    "\n",
    "\n",
    "\n",
    "|Name     | Description |\n",
    "|:--------:|:--------:|\n",
    "|  Similarity | Uses semantic similarity between inputs and examples to decide which examples to choose.   |  \n",
    "|  MMR  | Uses Max Marginal Relevance between inputs and examples to decide which examples to choose.  | \n",
    "| Length  |  Selects examples based on how many can fit within a certain length   |\n",
    "| Ngram  |  Uses ngram overlap between inputs and examples to decide which examples to choose.      |       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da61069-f9e7-497b-ba7d-0d31efdaf178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_chroma\n",
      "  Obtaining dependency information for langchain_chroma from https://files.pythonhosted.org/packages/10/05/34b30ff33af5ea7e6e5b6d1bf8ea3a0f2c1462c6b7f750f21dd0179fdf1e/langchain_chroma-0.1.2-py3-none-any.whl.metadata\n",
      "  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: chromadb<0.6.0,>=0.4.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_chroma) (0.4.15)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_chroma) (0.104.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.40 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_chroma) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_chroma) (1.26.0)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.3.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.7.3)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.23.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.8.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.16.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.20.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (6.1.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.59.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.0.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (28.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (8.2.3)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.40->langchain_chroma) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.40->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.40->langchain_chroma) (0.1.82)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.3,>=0.1.40->langchain_chroma) (23.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi<1,>=0.95.2->langchain_chroma) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi<1,>=0.95.2->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.40->langchain_chroma) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.30.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3<2.0,>=1.24.2 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.26.16)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.40->langchain_chroma) (3.10.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (23.5.26)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.24.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (6.8.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.61.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.20.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.20.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.41b0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.41b0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.28->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.65.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (12.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.15.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2023.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.17.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.5.0)\n",
      "Downloading langchain_chroma-0.1.2-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: langchain_chroma\n",
      "Successfully installed langchain_chroma-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069a6eb-f76e-4b4e-bbd4-7c74948e68c7",
   "metadata": {},
   "source": [
    "#### Select by similarity\n",
    "This object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb271cee-9e9a-4963-9c57-57f15ff8010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc910e55-20c6-4696-bf07-7ab31c62a7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input: worried\\nOutput: **calm** \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\"),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # The number of examples to produce.\n",
    "    k=1,\n",
    ")\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input is a feeling, so should select the happy/sad example\n",
    "prompt = similar_prompt.format(adjective=\"worried\") \n",
    "result = llm.invoke(prompt)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b069749-0328-4b0f-acd5-3cf0c5197678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: enthusiastic\n",
      "Output: apathetic\n",
      "\n",
      "Input: passionate\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# You can add new examples to the SemanticSimilarityExampleSelector as well\n",
    "similar_prompt.example_selector.add_example(\n",
    "    {\"input\": \"enthusiastic\", \"output\": \"apathetic\"}\n",
    ")\n",
    "print(similar_prompt.format(adjective=\"passionate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40089fa-d48c-471f-87c5-fca6d86bc933",
   "metadata": {},
   "source": [
    "#### Select by maximal marginal relevance (MMR)\n",
    "The MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd16e36b-e23c-4687-a31e-d164435f2df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Obtaining dependency information for faiss-cpu from https://files.pythonhosted.org/packages/4c/e1/657eb537027b2d7aa0f0ccfc58aee6fe0252ea3d9e49472aecc5c7f30992/faiss_cpu-1.8.0.post1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (1.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dpokh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.6 MB 330.3 kB/s eta 0:00:45\n",
      "   ---------------------------------------- 0.1/14.6 MB 656.4 kB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.2/14.6 MB 1.3 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.6/14.6 MB 3.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.2/14.6 MB 5.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.1/14.6 MB 7.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.1/14.6 MB 13.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.3/14.6 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.9/14.6 MB 18.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.9/14.6 MB 17.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.7/14.6 MB 20.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.9/14.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.5/14.6 MB 32.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.6 MB 32.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.6 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.6/14.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 22.5 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1347de9-4628-489d-9efe-672453a3c7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give the antonym of every input\\n\\nInput: happy\\nOutput: sad\\n\\nInput: windy\\nOutput: calm\\n\\nInput: worried\\nOutput:'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.example_selectors import  MaxMarginalRelevanceExampleSelector \n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "mmr_example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\"),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=mmr_example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# Input is a feeling, so should select the happy/sad example\n",
    "prompt = mmr_prompt.format(adjective=\"worried\") \n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5725435-8037-4054-ad85-9979fc9a6840",
   "metadata": {},
   "source": [
    "#### Select by length\n",
    "This example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d3d9ef2-e8c5-48fe-be25-4086a0dcce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: energetic\n",
      "Output: lethargic\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: big\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    # The examples it has available to choose from.\n",
    "    examples=examples,\n",
    "    # The PromptTemplate being used to format the examples.\n",
    "    example_prompt=example_prompt,\n",
    "    # The maximum length that the formatted examples should be.\n",
    "    # Length is measured by the get_text_length function below.\n",
    "    max_length=25,\n",
    "    # The function used to get the length of a string, which is used\n",
    "    # to determine which examples to include. It is commented out because\n",
    "    # it is provided as a default value if none is specified.\n",
    "    # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "# An example with small input, so it selects all examples.\n",
    "print(dynamic_prompt.format(adjective=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e104ab-0e26-4345-95c0-733503f1775b",
   "metadata": {},
   "source": [
    "#### Select by n-gram overlap\n",
    "The NGramOverlapExampleSelector selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.\n",
    "\n",
    "The selector allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5619c682-903b-4717-a79e-62362b46a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.example_selectors.ngram_overlap import NGramOverlapExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of a fictional translation task.\n",
    "examples = [\n",
    "    {\"input\": \"See Spot run.\", \"output\": \"Ver correr a Spot.\"},\n",
    "    {\"input\": \"My dog barks.\", \"output\": \"Mi perro ladra.\"},\n",
    "    {\"input\": \"Spot can run.\", \"output\": \"Spot puede correr.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d7f0a21-d23f-4cae-a844-417eff429dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the Spanish translation of every input\n",
      "\n",
      "Input: Spot can run.\n",
      "Output: Spot puede correr.\n",
      "\n",
      "Input: See Spot run.\n",
      "Output: Ver correr a Spot.\n",
      "\n",
      "Input: My dog barks.\n",
      "Output: Mi perro ladra.\n",
      "\n",
      "Input: Spot can run fast.\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "example_selector = NGramOverlapExampleSelector(\n",
    "    # The examples it has available to choose from.\n",
    "    examples=examples,\n",
    "    # The PromptTemplate being used to format the examples.\n",
    "    example_prompt=example_prompt,\n",
    "    # The threshold, at which selector stops.\n",
    "    # It is set to -1.0 by default.\n",
    "    threshold=-1.0,\n",
    "    # For negative threshold:\n",
    "    # Selector sorts examples by ngram overlap score, and excludes none.\n",
    "    # For threshold greater than 1.0:\n",
    "    # Selector excludes all examples, and returns an empty list.\n",
    "    # For threshold equal to 0.0:\n",
    "    # Selector sorts examples by ngram overlap score,\n",
    "    # and excludes those with no ngram overlap with input.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the Spanish translation of every input\",\n",
    "    suffix=\"Input: {sentence}\\nOutput:\",\n",
    "    input_variables=[\"sentence\"],\n",
    ")\n",
    "# An example input with large ngram overlap with \"Spot can run.\"\n",
    "# and no overlap with \"My dog barks.\"\n",
    "print(dynamic_prompt.format(sentence=\"Spot can run fast.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1ed6c-420a-4469-9b55-280a8f716d52",
   "metadata": {},
   "source": [
    "### Example Selectors Use Cases\n",
    "This table encapsulates the core application scenarios and use cases for each type of example selector, highlighting their unique advantages in different contexts.\n",
    "\n",
    "| **Example Selector** | **Scenario**                          | **Use Case**                                                                                          |\n",
    "|----------------------|---------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **Similarity**       | Customer Support Chatbots             | Selecting semantically similar past queries and resolutions for relevant responses.                   |\n",
    "|                      | Personalized Learning                 | Providing tailored learning material by selecting examples similar to student queries.                |\n",
    "|                      | E-commerce Product Recommendations    | Recommending products based on semantic similarity to user browsing history.                          |\n",
    "|                      | Mental Health Chatbot                 | Providing contextually appropriate and empathetic responses based on similarity to previous cases.    |\n",
    "| **MMR**              | News Article Summarization            | Selecting relevant and diverse sentences to create comprehensive summaries.                           |\n",
    "|                      | Content Recommendation Systems        | Balancing relevance and novelty in content recommendations.                                           |\n",
    "|                      | Academic Research Paper Summarization | Ensuring summaries include both the most relevant and diverse points.                                 |\n",
    "|                      | Playlist Generation                   | Creating music playlists that match user preferences while introducing variety.                       |\n",
    "| **Length**           | SMS or Tweet Generation               | Ensuring generated content fits within character limits for platforms like SMS and Twitter.           |\n",
    "|                      | Mobile App Responses                  | Selecting concise responses that fit within screen space constraints.                                 |\n",
    "|                      | Real-Time Translation                 | Providing translations that are concise and fit the interface constraints.                            |\n",
    "|                      | Automated Report Generation           | Adhering to length requirements for readable and relevant summaries or reports.                       |\n",
    "| **Ngram**            | Legal Document Analysis               | Ensuring responses align with specific legal terminology and language.                                |\n",
    "|                      | Plagiarism Detection                  | Identifying text with high similarity to existing documents for accurate plagiarism detection.        |\n",
    "|                      | Code Autocompletion                   | Improving code suggestions by selecting snippets with high ngram overlap to the current input.        |\n",
    "|                      | Historical Text Analysis              | Identifying recurring themes or concepts in historical documents.                                     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec164457-dadc-4b19-9da4-46b31af4c6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32502d7-7239-496d-8595-8fe780757e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
