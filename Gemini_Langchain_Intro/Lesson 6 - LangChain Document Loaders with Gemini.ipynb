{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506c390c-7114-4c2e-8375-5a9dc14c6795",
   "metadata": {},
   "source": [
    "## Document loaders\n",
    "Document Loaders in LangChain help in loading documents from a variety of structured/Unstructured sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe1b6b1-6e23-44ca-a59e-f3184210c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable pip version check\n",
    "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cf424b-4284-4cf0-a588-3082edfaa93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cff933c-dbd9-4580-abf6-0b25ecb0e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import  ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0.6) # \"chat-bison@001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad52f6-ff31-4c13-b11e-1935d73802d6",
   "metadata": {},
   "source": [
    "#### Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ca1234-a1a1-48fa-913f-f4fc69eb0a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='setosa: 1.4\\nversicolor: 0.2\\nvirginica: 0' metadata={'source': 'Data/iris.csv', 'row': 0}\n",
      "page_content='setosa: 1.4\\nversicolor: 0.2\\nvirginica: 0' metadata={'source': 'Data/iris.csv', 'row': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "file_path = 'Data/iris.csv'\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path=file_path)\n",
    "data = loader.load()\n",
    "\n",
    "for record in data[:2]:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f4b5a04-725f-49f9-a6a7-9ee16a9a8636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='setosa: setosa\\nversicolor: versicolor\\nvirginica: virginica' metadata={'source': 'Data/iris.csv', 'row': 0}\n",
      "page_content='setosa: 1.4\\nversicolor: 0.2\\nvirginica: 0' metadata={'source': 'Data/iris.csv', 'row': 1}\n"
     ]
    }
   ],
   "source": [
    "# Customizing Parsing \n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=file_path,\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\"setosa\", \"versicolor\", \"virginica\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "for record in data[:2]:\n",
    "    print(record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bfaf7-dff9-4c2e-9163-732ce73d36ff",
   "metadata": {},
   "source": [
    "##### Specifying a column as  document source\n",
    "The \"source\" key on Document metadata can be set using a column of the CSV. Use the source_column argument to specify a source for the document created from each row. Otherwise file_path will be used as the source for all documents created from the CSV file.\n",
    "This is useful when using documents loaded from CSV files for chains that answer questions using sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d50519c-8d48-4cd6-af0b-b1459556370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='setosa: 1.4\\nversicolor: 0.2\\nvirginica: 0' metadata={'source': '0', 'row': 0}\n",
      "page_content='setosa: 1.4\\nversicolor: 0.2\\nvirginica: 0' metadata={'source': '0', 'row': 1}\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=file_path, source_column=\"virginica\")\n",
    "\n",
    "data = loader.load()\n",
    "for record in data[:2]:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee7b4d5-2d59-4700-9045-844def72b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98' metadata={'source': 'C:\\\\Users\\\\dpokh\\\\AppData\\\\Local\\\\Temp\\\\tmpkevgitpw', 'row': 0}\n",
      "page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97' metadata={'source': 'C:\\\\Users\\\\dpokh\\\\AppData\\\\Local\\\\Temp\\\\tmpkevgitpw', 'row': 1}\n"
     ]
    }
   ],
   "source": [
    "# python's tempfile can be used when working with CSV strings directly.\n",
    "import tempfile\n",
    "from io import StringIO\n",
    "\n",
    "string_data = \"\"\"\n",
    "\"Team\", \"Payroll (millions)\", \"Wins\"\n",
    "\"Nationals\",     81.34, 98\n",
    "\"Reds\",          82.20, 97\n",
    "\"Yankees\",      197.96, 95\n",
    "\"Giants\",       117.62, 94\n",
    "\"\"\".strip()\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, mode=\"w+\") as temp_file:\n",
    "    temp_file.write(string_data)\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "loader = CSVLoader(file_path=temp_file_path)\n",
    "data = loader.load()\n",
    "for record in data[:2]:\n",
    "    print(record)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84063f01-b920-40bc-93dd-a00406a8a579",
   "metadata": {},
   "source": [
    "#### Loading from a directory\n",
    "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\n",
    "\n",
    "    How to load from a filesystem, including use of wildcard patterns;\n",
    "    How to use multithreading for file I/O;\n",
    "    How to use custom loader classes to parse specific file types (e.g., code);\n",
    "    How to handle errors, such as those due to decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0f892-f7ab-4f89-b500-84551a25d1e1",
   "metadata": {},
   "source": [
    "Install the Python SDK to support all document types with <b> pip install \"unstructured[all-docs]\" </b>\n",
    "\n",
    "For plain text files, HTML, XML, JSON and Emails that do not require any extra dependencies, you can run <b> pip install unstructured </b>\n",
    "\n",
    "To process other doc types, you can install the extras required for those documents, such as <b>pip install \"unstructured[docx,pptx]\" </b>\n",
    "Install the following system dependencies if they are not already available on your system. \n",
    "\n",
    "Depending on what document types you're parsing, you may not need all of these.\n",
    "\n",
    "libmagic-dev (filetype detection)\n",
    "\n",
    "poppler-utils (images and PDFs)\n",
    "\n",
    "tesseract-ocr (images and PDFs, install tesseract-lang for additional language support)\n",
    "\n",
    "libreoffice (MS Office docs)\n",
    "\n",
    "pandoc (EPUBs, RTFs and Open Office docs). Please note that to handle RTF files, you need version 2.14.2 or newer. Running either make install-pandoc or ./scripts/install-pandoc.sh will install the correct version for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fde903c-fa95-4ebb-b22a-27ca48753872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"Markdown_Dir\", glob=\"**/*.md\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3c6ef9-73b4-46b5-8ede-ad0b0f041547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(markdown)=\n",
      "\n",
      "Markdown\n",
      "\n",
      "Introduction\n",
      "\n",
      "In this chapter, you'll meet the lightweight markup language ca\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c23a5-1fc8-4db1-83df-7fdaa3c0df47",
   "metadata": {},
   "source": [
    "##### Show a progress bar\n",
    "By default a progress bar will not be shown. To show a progress bar, install the tqdm library (e.g. pip install tqdm), and set the show_progress parameter to True.\n",
    "\n",
    "##### Use multithreading\n",
    "By default the loading happens in one thread. In order to utilize several threads set the use_multithreading flag to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "341f9f02-e2e7-4f38-9c78-3a7fc60d5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af3f104-eb74-4356-a297-3cc8d007f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 11.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"../\", glob=\"**/*.md\", show_progress=True, use_multithreading =True ) # Load all the MD files\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdb0f8-c713-43c2-bd2a-0fec7b33ba8c",
   "metadata": {},
   "source": [
    "##### Change loader class\n",
    "By default this uses the UnstructuredLoader class. To customize the loader, specify the loader class in the loader_cls kwarg. Below we show an example using TextLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a60188e-82e1-499d-9df6-5cf8a6fc31b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# langchain-gemini\n",
      "Gemini 101: Code along with Langchain to explore Google's AI magic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\"../\", glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf01fc-eb41-469d-87cb-e724f1ba6534",
   "metadata": {},
   "source": [
    "If you need to load Python source code files, use the PythonLoader:\n",
    "   ```python\n",
    "   \n",
    "        from langchain_community.document_loaders import PythonLoader\n",
    "        \n",
    "        loader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644aae7-40a4-4507-9a25-dbbdea74e49c",
   "metadata": {},
   "source": [
    "##### Silent fail\n",
    "We can pass the parameter silent_errors to the DirectoryLoader to skip the files which could not be loaded and continue the load process.\n",
    "\n",
    "##### Auto detect encodings\n",
    "We can also ask TextLoader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6720b43-9582-4e76-8e47-41fa793f9d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data\\\\Api.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Data/\"\n",
    "text_loader_kwargs = {\"autodetect_encoding\": True}\n",
    "loader = DirectoryLoader(\n",
    "    path, glob=\"**/*.txt\", loader_cls=TextLoader, silent_errors=True,loader_kwargs=text_loader_kwargs\n",
    ")\n",
    "docs = loader.load()\n",
    "doc_sources = [doc.metadata[\"source\"] for doc in docs]\n",
    "doc_sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be816e55-dbce-45bb-88d7-6cd6e1059f19",
   "metadata": {},
   "source": [
    "#### Parsing/Loading HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011c417-bb7d-4f7c-8a16-662b55308f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading HTML with Unstructured\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "htmlpath = \"Data/mdguide.html\"\n",
    "loader = UnstructuredHTMLLoader(htmlpath) # Load all the MD files\n",
    "data= loader.load()\n",
    "len(data)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198f6138-c586-4dd7-af19-70e9e3626bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading HTML with BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d821fc4-1d13-476a-af3b-ff6e7e79d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b22a8e-a70e-4cf7-8b33-adfded80e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "loader = BSHTMLLoader(htmlpath,open_encoding = 'utf-8')\n",
    "data = loader.load()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dae3e3-20d6-429c-9cd4-5d413ce7911d",
   "metadata": {},
   "source": [
    "#### Loading JSON\n",
    "\n",
    "JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n",
    "\n",
    "JSON Lines is a file format where each line is a valid JSON value.\n",
    "\n",
    "LangChain implements a JSONLoader to convert JSON and JSONL data into LangChain Document objects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n",
    "\n",
    "It uses the jq python package. Check out this manual for a detailed documentation of the jq syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88c498-947f-4770-aac7-4f5904bdbb32",
   "metadata": {},
   "source": [
    "!pip install jq -q\n",
    "\n",
    "!pip install pyjq -q\n",
    "\n",
    "jq cannot be installed in windows. So navigate to https://jeffreyknockel.com/jq/ - download the .whl file as per your python version and pip install it \n",
    "\n",
    "```python\n",
    "!python -m pip install \"C:\\Users\\dpokh\\Downloads\\jq-1.4.0-cp311-cp311-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d2ca00-7a21-4068-827e-3e9f43bea7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_still_participant': True,\n",
      " 'messages': [{'content': \"That's rough, buddy.\",\n",
      "               'sender_name': 'Zuko',\n",
      "               'timestamp_ms': 1579137191303,\n",
      "               'type': 'Generic'},\n",
      "              {'content': 'My first girlfriend turned into the moon',\n",
      "               'sender_name': 'Sokka',\n",
      "               'timestamp_ms': 1579137103044,\n",
      "               'type': 'Generic'},\n",
      "              {'content': \"Everyone in the Fire Nation thinks I'm a traitor, I \"\n",
      "                          \"couldn't drag her into it.\",\n",
      "               'sender_name': 'Zuko',\n",
      "               'timestamp_ms': 1579137078312,\n",
      "               'type': 'Generic'},\n",
      "              {'content': 'Yeah',\n",
      "               'sender_name': 'Zuko',\n",
      "               'timestamp_ms': 1579136858575,\n",
      "               'type': 'Generic'},\n",
      "              {'content': 'That gloomy girl who sighs a lot?!',\n",
      "               'sender_name': 'Sokka',\n",
      "               'timestamp_ms': 1579136847743,\n",
      "               'type': 'Generic'},\n",
      "              {'content': 'Well, I did have a girlfriend. Mai.',\n",
      "               'sender_name': 'Zuko',\n",
      "               'timestamp_ms': 1579136839127,\n",
      "               'type': 'Generic'},\n",
      "              {'content': \"Really? You didn't leave behind anyone you cared \"\n",
      "                          'about?',\n",
      "               'sender_name': 'Sokka',\n",
      "               'timestamp_ms': 1579136837474,\n",
      "               'type': 'Generic'},\n",
      "              {'content': \"It wasn't that hard\",\n",
      "               'sender_name': 'Zuko',\n",
      "               'timestamp_ms': 1579136836700,\n",
      "               'type': 'Generic'},\n",
      "              {'content': 'I think your uncle would be proud of you, leaving '\n",
      "                          \"your home to come help us -- that's hard\",\n",
      "               'sender_name': 'Sokka',\n",
      "               'timestamp_ms': 1579136824886,\n",
      "               'type': 'Generic'}],\n",
      " 'participants': [{'name': 'Zuko'}, {'name': 'Sokka'}],\n",
      " 'thread_path': 'inbox/TheBoilingRockPt1_u3hhb2HmZg',\n",
      " 'thread_type': 'Regular',\n",
      " 'title': 'The Boiling Rock Pt. 1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "json_path=\"Data/json_sample.json\"\n",
    "data = json.loads(Path(json_path).read_text())\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe795f-be6d-450f-9917-1619e7faf31e",
   "metadata": {},
   "source": [
    "Extracting the values under the content field within the messages key of the JSON data  can easily be done through the JSONLoader as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70db356e-a37d-43ff-aa81-56135bff0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path= json_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f8de2ab-4b4e-4ef7-b7a0-dbfcf6f02d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"That's rough, buddy.\", metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 1}),\n",
      " Document(page_content='My first girlfriend turned into the moon', metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 2}),\n",
      " Document(page_content=\"Everyone in the Fire Nation thinks I'm a traitor, I couldn't drag her into it.\", metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 3}),\n",
      " Document(page_content='Yeah', metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 4}),\n",
      " Document(page_content='That gloomy girl who sighs a lot?!', metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 5}),\n",
      " Document(page_content='Well, I did have a girlfriend. Mai.', metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 6}),\n",
      " Document(page_content=\"Really? You didn't leave behind anyone you cared about?\", metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 7}),\n",
      " Document(page_content=\"It wasn't that hard\", metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 8}),\n",
      " Document(page_content=\"I think your uncle would be proud of you, leaving your home to come help us -- that's hard\", metadata={'source': 'C:\\\\Users\\\\dpokh\\\\langchain-gemini\\\\langchain-gemini\\\\Gemini_Langchain_Intro\\\\Data\\\\json_sample.json', 'seq_num': 9})]\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dc104-a807-4de6-aa31-9759ac5a28cf",
   "metadata": {},
   "source": [
    "##### Loading Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a335630-08ad-4f37-b1ed-7f15dd4e3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-gemini\n",
      "\n",
      "Gemini 101: Code along with Langchain to explore Google's AI magic.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "markdown_path = \"../README.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "data = loader.load()\n",
    "assert len(data) == 1\n",
    "assert isinstance(data[0], Document)\n",
    "readme_content = data[0].page_content\n",
    "print(readme_content[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeeeb93c-1d35-4000-aba5-17977aad7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2\n",
      "\n",
      "page_content='langchain-gemini' metadata={'source': '../README.md', 'category_depth': 0, 'last_modified': '2024-07-02T09:55:35', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '..', 'filename': 'README.md', 'category': 'Title'}\n",
      "\n",
      "page_content=\"Gemini 101: Code along with Langchain to explore Google's AI magic.\" metadata={'source': '../README.md', 'last_modified': '2024-07-02T09:55:35', 'languages': ['eng'], 'parent_id': '8e5c643ea375992feaa4406a0d53d353', 'filetype': 'text/markdown', 'file_directory': '..', 'filename': 'README.md', 'category': 'NarrativeText'}\n",
      "\n",
      "{'NarrativeText', 'Title'}\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")\n",
    "\n",
    "data = loader.load()\n",
    "print(f\"Number of documents: {len(data)}\\n\")\n",
    "\n",
    "for document in data[:2]:\n",
    "    print(f\"{document}\\n\")\n",
    "\n",
    "print(set(document.metadata[\"category\"] for document in data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f4e2e-6872-458c-b759-eb951495dcbe",
   "metadata": {},
   "source": [
    "##### Loading Portable Document Format (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "301cea26-cdcf-435b-a75c-564cf22b870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c30fa8fb-ff90-47d3-ba0c-0f2c86f6f46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021', metadata={'source': 'Data/layout_parser.pdf', 'page': 0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\n",
    "  \"Data/layout_parser.pdf\"\n",
    ")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bce5e-ab97-4a2e-8148-87de96d2c4f9",
   "metadata": {},
   "source": [
    "##### Extract text from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8844a3b-2d43-4ae5-b168-cdfbe1537b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3996f42a-6a5e-4b50-b102-a5c9eeaf606c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\\nTable 1: Current layout detection models in the LayoutParser model zoo\\nDataset Base Model1Large Model Notes\\nPubLayNet [38] F / M M Layouts of modern scientiﬁc documents\\nPRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\\nNewspaper [17] F - Layouts of scanned US newspapers from the 20th century\\nTableBank [18] F F Table region on modern scientiﬁc and business document\\nHJDataset [31] F / M - Layouts of history Japanese documents\\n1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\\nvs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\\nbackbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\\nR-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\\nzoo in coming months.\\nlayout data structures , which are optimized for eﬃciency and versatility. 3) When\\nnecessary, users can employ existing or customized OCR models via the uniﬁed\\nAPI provided in the OCR module . 4)LayoutParser comes with a set of utility\\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\\nis also highly customizable, via its integration with functions for layout data\\nannotation and model training . We now provide detailed descriptions for each\\ncomponent.\\n3.1 Layout Detection Models\\nInLayoutParser , a layout model takes a document image as an input and\\ngenerates a list of rectangular boxes for the target content regions. Diﬀerent\\nfrom traditional methods, it relies on deep convolutional neural networks rather\\nthan manually curated rules to identify content regions. It is formulated as an\\nobject detection problem and state-of-the-art models like Faster R-CNN [ 28] and\\nMask R-CNN [ 12] are used. This yields prediction results of high accuracy and\\nmakes it possible to build a concise, generalized interface for layout detection.\\nLayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\\nperform layout detection with only four lines of code in Python:\\n1import layoutparser as lp\\n2image = cv2. imread (\" image_file \") # load images\\n3model = lp. Detectron2LayoutModel (\\n4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\\n5layout = model . detect ( image )\\nLayoutParser provides a wealth of pre-trained model weights using various\\ndatasets covering diﬀerent languages, time periods, and document types. Due to\\ndomain shift [ 7], the prediction performance can notably drop when models are ap-\\nplied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\\ndocument structures and layouts vary greatly in diﬀerent domains, it is important\\nto select models trained on a dataset similar to the test samples. A semantic syntax\\nis used for initializing the model weights in LayoutParser , using both the dataset\\nname and model name lp://<dataset-name>/<model-architecture-name> .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path, extract_images=True)\n",
    "pages = loader.load()\n",
    "pages[4].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49942897-83b7-436e-826f-85440852ee3b",
   "metadata": {},
   "source": [
    "##### Using PyMuPDF\n",
    "PyMuPDF is optimized for speed, and contains detailed metadata about the PDF and its pages. It returns one document per page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23e41b2f-c2e4-4a73-b4c7-00bb340e067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c7ffdea-43c0-4f93-a6bc-32cedff6a3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n', metadata={'source': 'Data/layout_parser.pdf', 'file_path': 'Data/layout_parser.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(file_path\n",
    ")\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27269e99-1d6b-454a-9be5-85ed393bfadf",
   "metadata": {},
   "source": [
    "##### Using pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe289c9-6425-4710-ae9c-81f856ccabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet pdfminer pdfminer.six\n",
    "!pip install --upgrade --quiet pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6e3ef-dc0d-4b22-b192-de8867f3974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "file_path = (\n",
    "  \"Data/layout_parser.pdf\"\n",
    ")\n",
    "loader = PDFMinerLoader(file_path)\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb52b2f-0788-48a4-bd86-905efbf1a8f9",
   "metadata": {},
   "source": [
    "###### Using PDFMiner to generate HTML text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34234796-6fad-4f32-a152-d5c8ad6d4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "loader = PDFMinerPDFasHTMLLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe28bbc-3791-42f2-a42c-b05b2a997050",
   "metadata": {},
   "source": [
    "##### Loading from PDF Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dc86946-ead7-4359-86fe-2810c17d8628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"Data/\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "#data[0]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c3a51-30c2-4f52-b486-8ba0463a1928",
   "metadata": {},
   "source": [
    "##### UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900436ee-6a59-4bc5-a15b-5950463f7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "###!pip install --quiet pillow-heif\n",
    "###!pip install matplotlib\n",
    "##!pip install unstructured-inference --user\n",
    "## !pip install'unstructured_pytesseract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8229bd-52bd-42f8-9659-5f5014ebc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(file_path, mode=\"elements\")\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea5851-bdbc-40f2-a1b3-a17380f4ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2302.03803.pdf\")\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296f32e-1e6f-4be6-95eb-db9fd316028f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
