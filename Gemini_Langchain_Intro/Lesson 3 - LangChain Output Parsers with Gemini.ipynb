{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b65b6ad-fd39-480f-9b3e-c55a00e0fe9c",
   "metadata": {},
   "source": [
    "## langchain Output Parsers with Gemini\n",
    "Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\n",
    "\n",
    "LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports: \n",
    "Sure, here's the information transformed into a tabular format:\n",
    "\n",
    "| **Name**            | **Supports Streaming** | **Has Format Instructions** | **Calls LLM** | **Input Type**    | **Output Type**          | **Description**                                                                                                                                   |\n",
    "|---------------------|------------------------|-----------------------------|---------------|-------------------|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **JSON**            | ✅                      | ✅                           |               | str \\| Message     | JSON object              | Returns a JSON object as specified. You can specify a Pydantic model, and it will return JSON for that model. Most reliable for structured data.   |\n",
    "| **XML**             | ✅                      | ✅                           |               | str \\| Message     | dict                     | Returns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).                      |\n",
    "| **CSV**             | ✅                      | ✅                           |               | str \\| Message     | List[str]                | Returns a list of comma-separated values.                                                                                                          |\n",
    "| **OutputFixing**    |                        | ✅                           | ✅            | str \\| Message     |                          | Wraps another output parser. If that output parser errors, it passes the error message and bad output to an LLM to fix the output.                 |\n",
    "| **RetryWithError**  |                        | ✅                           | ✅            | str \\| Message     |                          | Wraps another output parser. If that output parser errors, it passes the original inputs, bad output, and error message to an LLM to fix it.       |\n",
    "| **Pydantic**        | ✅                      |                             |               | str \\| Message     | pydantic.BaseModel       | Takes a user-defined Pydantic model and returns data in that format.                                                                               |\n",
    "| **YAML**            | ✅                      |                             |               | str \\| Message     | pydantic.BaseModel       | Takes a user-defined Pydantic model and returns data in that format using YAML to encode it.                                                      |\n",
    "| **PandasDataFrame** | ✅                      |                             |               | str \\| Message     | dict                     | Useful for doing operations with pandas DataFrames.                                                                                               |\n",
    "| **Enum**            | ✅                      |                             |               | str \\| Message     | Enum                     | Parses response into one of the provided enum values.                                                                                             |\n",
    "| **Datetime**        | ✅                      |                             |               | str \\| Message     | datetime.datetime        | Parses response into a datetime string.                                                                                                           |\n",
    "| **Structured**      | ✅                      |                             |               | str \\| Message     | Dict[str, str]           | Returns structured information. Less powerful than other parsers since it only allows for string fields. Useful with smaller LLMs.                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c3af26-4b52-433d-842f-b349ce952ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable pip version check\n",
    "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11714c7f-2e83-4fad-ae54-cae76d92acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b8abb-0977-4b7c-a526-a7f3de7ee563",
   "metadata": {},
   "source": [
    "#### JSON Outputparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430e5527-dac4-45ee-b416-14e562441841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms? \",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai.chat_models import  ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "model= ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\",max_tokens_to_sample=512, temperature = 0) # \"chat-bison@001\"\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724833a-6117-4690-ab75-4b5fe953beae",
   "metadata": {},
   "source": [
    "#### XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69eb7410-0364-471a-8c2c-4eee55d5a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movies': [{'actor': [{'name': 'Amir Khan'}, {'film': [{'name': 'Lagaan: Once Upon a Time in India'}, {'genre': 'Drama, Musical, Sport'}]}, {'film': [{'name': 'Rang De Basanti'}, {'genre': 'Drama, Musical'}]}, {'film': [{'name': '3 Idiots'}, {'genre': 'Comedy, Drama'}]}, {'film': [{'name': 'Dhoom 3'}, {'genre': 'Action, Thriller'}]}, {'film': [{'name': 'PK'}, {'genre': 'Comedy, Drama, Science Fiction'}]}, {'film': [{'name': 'Dangal'}, {'genre': 'Biography, Drama, Sport'}]}, {'film': [{'name': 'Secret Superstar'}, {'genre': 'Drama, Musical'}]}, {'film': [{'name': 'Thugs of Hindostan'}, {'genre': 'Action, Adventure, History'}]}, {'film': [{'name': 'Laal Singh Chaddha'}, {'genre': 'Comedy, Drama, Romance'}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "parser = XMLOutputParser(tags=[\"movies\", \"actor\", \"film\", \"name\", \"genre\"])\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"{query}\\n{format_instructions}\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Amir Khan.\"\n",
    "chain = prompt | model | parser\n",
    "\n",
    "output = chain.invoke({\"query\": actor_query})\n",
    "\n",
    "print(output)\n",
    "\n",
    "# We will add these instructions to the prompt below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4c7ac-b46b-4f05-8530-a1a52602a427",
   "metadata": {},
   "source": [
    "#### CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00197634-3d09-4e4e-9552-9d493c884800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chocolate', 'vanilla', 'strawberry', 'mint chocolate chip', 'cookie dough']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "csvchain = prompt | model | output_parser\n",
    "csvchain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf26ad6-7212-4334-8631-5dbd659f6bd9",
   "metadata": {},
   "source": [
    "#### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a899c707-6e30-4240-ac5c-b15a9298d80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the Nepalese climber bring a ladder to Mount Everest?', punchline='He heard it was a really tall mountain.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "        # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\"query\": \"Tell me a joke about Nepal.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91b059-7dd2-4ea0-b1de-cfd76d7eb96f",
   "metadata": {},
   "source": [
    "#### YamlOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbbbeed6-3db3-49de-b210-8aa2062258cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the Nepalese man bring a ladder to the mountain?', punchline='He wanted to get to the top of the world!')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up a parser + inject instructions into the prompt template.\n",
    "from langchain.output_parsers.yaml import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke about Nepal.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6102e-7e14-4322-92a1-e0a8e224b2d5",
   "metadata": {},
   "source": [
    "#### PandasDataFrameOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe82026-cd40-4efd-bc52-e1a4e606c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Define your desired Pandas DataFrame.\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"num_legs\": [2, 4, 8, 0],\n",
    "        \"num_wings\": [2, 0, 0, 0],\n",
    "        \"num_specimen_seen\": [10, 2, 1, 8],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887fc0af-a90b-477b-a92e-90ebab309ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_wings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_wings\n",
       "0          2\n",
       "1          0\n",
       "2          0\n",
       "3          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of a column operation being performed.\n",
    "df_query = \"Retrieve the num_wings column.\"\n",
    "\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "parser_output = pd.DataFrame(parser_output)\n",
    "parser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7771251-6413-4921-8fec-0645b3430362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of a random Pandas DataFrame operation limiting the number of rows\n",
    "df_query = \"Retrieve the average of the num_legs column from rows 1 to 3.\"\n",
    "\n",
    "# Set up the prompt.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "\n",
    "print(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da25d4d-a370-4aa9-ad37-38a37820c358",
   "metadata": {},
   "source": [
    "#### EnumOutputparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b78296-4c4c-4f8f-8555-75e255747d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Colors(Enum):\n",
    "    RED = \"red\"\n",
    "    GREEN = \"green\"\n",
    "    BLUE = \"blue\"\n",
    "    BROWN = \"brown\"\n",
    "\n",
    "parser = EnumOutputParser(enum=Colors,return_exceptions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4da84ce-8ab8-47d0-820e-0222752a866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Colors.BROWN: 'brown'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"What color eyes does this person have?\n",
    "\n",
    "> Person: {person}\n",
    "\n",
    "Instructions: {instructions}\"\"\"\n",
    ").partial(instructions=parser.get_format_instructions())\n",
    "chain = prompt | ChatGoogleGenerativeAI(model = \"gemini-pro\") | parser\n",
    "chain.invoke({\"person\": \"Salman Khan\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce3680-6095-4092-89b3-dff5cc61f5e0",
   "metadata": {},
   "source": [
    "#### DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398e39d9-f27a-431c-afaf-ef67baaf4dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output: 1945-10-24 00:00:00\n",
      "Formatted Date: 10/24/1945\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | ChatGoogleGenerativeAI(model = \"gemini-pro\")| output_parser\n",
    "output = chain.invoke({\"question\": \"when was the United Nations founded?\"})\n",
    "print( \"Raw Output:\", output)\n",
    "from datetime import datetime\n",
    "\n",
    "# Given datetime object\n",
    "\n",
    "# Format the datetime object into the desired MM/DD/YYYY format\n",
    "formatted_date = output.strftime('%m/%d/%Y')\n",
    "\n",
    "print(\"Formatted Date:\", formatted_date) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c8a3a-1c79-4347-8e3c-70887aeb8539",
   "metadata": {},
   "source": [
    "#### Structured output parser\n",
    "This output parser can be used when you want to return multiple fields. While the <b>  Pydantic/JSON parser is more powerful </b>, this is useful for less powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbcf21d0-a7e4-4853-8dca-8da7a0c768af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The capital of Nepal is Kathmandu.',\n",
       " 'source': 'https://www.worldpopulationreview.com/countries/nepal-population'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(\n",
    "        name=\"source\",\n",
    "        description=\"source used to answer the user's question, should be a website.\",\n",
    "    ),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"what's the capital of Nepal?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c248094-d73b-4acc-82d7-048be94bd9e8",
   "metadata": {},
   "source": [
    "#### Output-fixing parser\n",
    "This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n",
    "\n",
    "But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "508586d9-0019-4140-803f-b8906cc9bebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'NoneType' object has no attribute 'invoke'\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.llms import OpenAI  # or the appropriate model from langchain\n",
    "from langchain_google_genai.llms import GoogleGenerativeAI\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\")\n",
    "\n",
    "# Define your Pydantic model\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")\n",
    "\n",
    "actor_query = \"Generate the filmography for a random actor.\"\n",
    "\n",
    "# Initialize PydanticOutputParser with Actor model\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "# Misformatted JSON string (not valid JSON)\n",
    "misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\"\n",
    "\n",
    "# Initialize OutputFixingParser with the PydanticOutputParser and LLM model\n",
    "new_parser = OutputFixingParser(parser=parser, llm=llm)\n",
    "\n",
    "# Parsing the misformatted input\n",
    "try:\n",
    "    parsed_output = new_parser.parse(misformatted)\n",
    "    print(parsed_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ec8ff-70ed-4961-9230-8dd58d6ef7d5",
   "metadata": {},
   "source": [
    "#### Retry parser\n",
    "While in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be5beb4-cb9b-4d67-8770-2245f06a8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import (\n",
    "    OutputFixingParser,\n",
    "    PydanticOutputParser,\n",
    ")\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "template = \"\"\"Based on the user question, provide an Action and Action Input for what step should be taken.\n",
    "{format_instructions}\n",
    "Question: {query}\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "class Action(BaseModel):\n",
    "    action: str = Field(description=\"action to take\")\n",
    "    action_input: str = Field(description=\"input to the action\")\n",
    "\n",
    "\n",
    "retry_parser = PydanticOutputParser(pydantic_object=Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e8169c0-83a4-4203-9af2-9380304e05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": retry_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1447e138-d3d2-40b6-9350-0537d659820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"action\": {\"title\": \"Action\", \"description\": \"action to take\", \"type\": \"string\"}, \"action_input\": {\"title\": \"Action Input\", \"description\": \"input to the action\", \"type\": \"string\"}}, \"required\": [\"action\", \"action_input\"]}\\n```\\nwho is leo di caprios gf?\\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.format_prompt(query=\"who is leo di caprios gf?\")\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4b77fc5-fa51-4560-8e61-e32417b12c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_response = '{\"action\": \"search\"}'\n",
    "# parser.parse(bad_response)\n",
    "\n",
    "from langchain.output_parsers.retry import RetryOutputParser\n",
    "\n",
    "retry_parser = RetryOutputParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a56dff-d877-449c-b9f9-1470bcdf4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_parser.parse_with_prompt(bad_response, prompt_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
