{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63d93cc-a7b9-472c-971d-4a420ef0796e",
   "metadata": {},
   "source": [
    "## Text splitters\n",
    "Splitting a long document into smaller chunks that can fit into your model's context window is a necessary task. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "#### Levels Of Text Splitting\n",
    "(Taken from : https://github.com/FullStackRetrieval-com)\n",
    "\n",
    "<b>Level 1: Character Splitting </b>- Simple static character chunks of data\n",
    "\n",
    "<b>Level 2: Recursive Character Text Splitting </b>- Recursive chunking based on a list of separators\n",
    "\n",
    "<b>Level 3: Document Specific Splitting - </b> Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "\n",
    "<b>Level 4: Semantic Splitting - </b> Embedding walk based chunking\n",
    "\n",
    "<b>Level 5: Agentic Splitting - </b> Experimental method of splitting text with an agent-like system.\n",
    "\n",
    "<b>Level 6: Alternative Representation Chunking + Indexing </b>- Derivative representations of your raw text that will aid in retrieval and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3635ca4-e4fe-4b46-b619-2c94e221e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable pip version check\n",
    "os.environ['PIP_DISABLE_PIP_VERSION_CHECK'] = '1'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8d225-40be-4edf-bce2-bcb5d7518265",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7d9f9-0ee0-4293-bad5-dccd80fda4de",
   "metadata": {},
   "source": [
    "##### Character Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78ccae8-f0c1-47f8-912c-70a3da5f3f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Request Structure: Test the correct formation and syntax of API requests, including headers, query parameters, and request bodies.\\n\\nResponse Validation: Verify the accuracy and completeness of the API responses. Validate the response status codes, headers, and payload.\\n\\nData Format and Encoding: Ensure that the API handles data formats correctly, such as JSON, XML, or others. Validate that the encoding and decoding processes work as expected.\\n\\nError Handling: Test how the API handles error scenarios and responds with appropriate error codes, messages, and error structures. Check if error conditions are handled gracefully.\\n\\nAuthentication and Authorization: Validate the authentication mechanisms provided by the API, such as API keys, tokens, or OAuth. Test different authentication scenarios and authorization levels to ensure access control is properly enforced.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load an example document\n",
    "text = \"Data/Api.txt\"\n",
    "with open(text) as f:\n",
    "    api_text = f.read()\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    strip_whitespace=False\n",
    ")\n",
    "texts = text_splitter.create_documents([api_text])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dc155-0cb7-4162-a269-f2291fceed8e",
   "metadata": {},
   "source": [
    "##### Recursive Character Text Splitting\n",
    "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
    "\n",
    "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
    "\n",
    "You can see the default separators for LangChain here. Let's take a look at them one by one.\n",
    "\n",
    "\"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "\n",
    "\"\\n\" - New lines\n",
    "\n",
    "\" \" - Spaces\n",
    "\n",
    "\"\" - Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03566fa5-ef37-4ae7-8ea8-b0861357c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Request Structure: Test the correct formation and syntax of API requests, including headers, query'\n",
      "page_content='headers, query parameters, and request bodies.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([api_text])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb7bd7-9ab0-4ccf-b11c-939ced7432b5",
   "metadata": {},
   "source": [
    "##### Split by HTML header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6a4d8-45c2-4624-b3d6-3a6861a2f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "\n",
    "fname = \"Data/mdguide.html\"\n",
    "\n",
    "HtmlFile = open(fname, 'r', encoding='utf-8')\n",
    "source_code = HtmlFile.read() \n",
    "\n",
    "headers_to_split_on = [\n",
    "    #(\"h1\", \"Header 1\"),\n",
    "    #(\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(source_code)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc924869-fd36-4813-a573-24387604bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "\n",
    "# for local file use html_splitter.split_text_from_file(<path_to_file>)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c466a-0bd5-45b6-b905-97e9f4344676",
   "metadata": {},
   "source": [
    "HTMLHeaderTextSplitter, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as RecursiveCharacterTextSplitter.\n",
    "\n",
    "This can be done using the .split_documents method of the second splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ee08ed8-fd67-40ce-8f48-1828d2167c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We see that G√∂del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (‚ÄúThis sentence is false‚Äù) and Berry‚Äôs paradox (‚ÄúThe least number not defined by an expression consisting of just fourteen English words‚Äù). G√∂del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth', metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='means that arithmetic truth and arithmetic provability are not co-extensive ‚Äî whence the First Incompleteness Theorem.', metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='This account of G√∂del‚Äôs discovery was told to Hao Wang very much after the fact; but in G√∂del‚Äôs contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See G√∂del 2003a and G√∂del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by G√∂del by 1931. But he neither publicized nor published the result; the biases logicians', metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to G√∂del‚Äôs publication of that theorem.', metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}),\n",
       " Document(page_content='We now describe the proof of the two theorems, formulating G√∂del‚Äôs results in Peano arithmetic. G√∂del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following G√∂del‚Äôs notation.', metadata={'Header 1': 'Kurt G√∂del', 'Header 2': '2. G√∂del‚Äôs Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n",
    "splits[80:85]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f424b2a-88f5-4059-8dbe-70698d50d939",
   "metadata": {},
   "source": [
    "##### Splitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0430bd1f-3dcc-4161-ba3a-579eacb20a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9bdfd0-8b03-4bfe-bbf3-ead505e50137",
   "metadata": {},
   "source": [
    "###### python splitters:\n",
    "\n",
    "    \\nclass - Classes first\n",
    "    \\ndef - Functions next\n",
    "    \\n\\tdef - Indented functions\n",
    "    \\n\\n - Double New lines\n",
    "    \\n - New Lines\n",
    "    \" \" - Spaces\n",
    "    \"\" - Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82ffb33e-2b10-4a38-9589-b7e4edcbf16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")\\n\\n# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "python_splitter.create_documents([PYTHON_CODE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99f0e374-6ebe-43ff-8ba2-d74cbd9f0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fad34da-7609-43d7-8894-cf772caa8907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# ü¶úÔ∏èüîó LangChain'),\n",
       " Document(page_content='‚ö° Building applications with LLMs through composability ‚ö°'),\n",
       " Document(page_content='## Quick Install\\n\\n```bash'),\n",
       " Document(page_content=\"# Hopefully this code block isn't split\"),\n",
       " Document(page_content='pip install langchain'),\n",
       " Document(page_content='```python\\ndef hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_text = \"\"\"\n",
    "# ü¶úÔ∏èüîó LangChain\n",
    "\n",
    "‚ö° Building applications with LLMs through composability ‚ö°\n",
    "\n",
    "## Quick Install\n",
    "\n",
    "```bash\n",
    "# Hopefully this code block isn't split\n",
    "pip install langchain\n",
    "\n",
    "\n",
    "```python\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\"\"\"\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "md_docs = md_splitter.create_documents([markdown_text])\n",
    "md_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e026e15f-2eee-41ca-83e0-37124f53a485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# ü¶úÔ∏èüîó LangChain\\n\\n‚ö° Building applications with LLMs through composability ‚ö°\\n\\n## Quick Install'),\n",
       " Document(page_content=\"```bash\\n# Hopefully this code block isn't split\\npip install langchain\\n\\n\\n```python\"),\n",
       " Document(page_content='def hello_world():\\n    print(\"Hello, World!\")\\n\\n# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "md_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "md_splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559b0cc-09df-4726-bcb1-3eac9c4633b7",
   "metadata": {},
   "source": [
    "##### Splitting PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e328cd4a-96e9-47b8-9c1e-14d8614d9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install unstructured_pytesseract -q\n",
    "! pip install pdf2image pypdfium2 -q\n",
    "## Install poppler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67239e78-097b-407b-a8e7-f31b3c5c4137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpokh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba8fc2-f7bb-4ea2-951d-5384812d793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Data/output_parser.pdf\"\n",
    "\n",
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\", \n",
    "    infer_table_structure=True, \n",
    "    model_name=\"yolox\"\n",
    ")\n",
    "elements\n",
    "elements[-4].metadata.text_as_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab77ffd-cc39-4c18-88f6-a0e927bfc703",
   "metadata": {},
   "source": [
    "##### split text based on semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332b4a8c-0965-473b-ae72-af4ded8260d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad19a84-bbca-41a5-85c9-a9d96e14f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia√¢¬Ä¬ôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament √¢¬Ä¬úLight will win over darkness.√¢¬Ä¬ù The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we√¢¬Ä¬ôve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "\n",
    "text = \"Data/state_of_the_union.txt\"\n",
    "with open(text,encoding = 'latin1') as f:\n",
    "    api_text = f.read()\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "docs = text_splitter.create_documents([api_text])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d383bf-232e-4eaf-9323-6c7427fb2b3f",
   "metadata": {},
   "source": [
    "###### Breakpoints\n",
    "This chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split. There are different kinds of breakpoints: \n",
    "1. Percentile\n",
    "The default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576bb19c-2335-461c-957b-bb932328c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings, breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb95a2d9-15ab-4fc0-9472-208b40a9992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([api_text])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5bdcd-4973-4c10-98d5-81b90579a5ea",
   "metadata": {},
   "source": [
    "2. Standard Deviation\n",
    "\n",
    "In this method, any difference greater than X standard deviations is split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41b055c-aeeb-4301-b6f3-7fd65f3681b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings, breakpoint_threshold_type=\"standard_deviation\"\n",
    ")\n",
    "docs = text_splitter.create_documents([api_text])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52a86a-5f94-44ea-8feb-63b792b4c87a",
   "metadata": {},
   "source": [
    "3. Interquartile\n",
    "\n",
    "In this method, the interquartile distance is used to split chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30d87a14-1280-4611-adce-308aa3de33f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings, breakpoint_threshold_type=\"interquartile\"\n",
    ")\n",
    "docs = text_splitter.create_documents([api_text])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715f52d-b479-4e67-ad3d-2cbcef04c592",
   "metadata": {},
   "source": [
    "4. Gradient\n",
    "\n",
    "In this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08f3a8c7-17e8-48c1-83d3-1c5b6c3902d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n",
      "Members of Congress and the Cabinet.\n",
      "Smartphones. The Internet. Technology we have yet to invent. But that√¢¬Ä¬ôs just the beginning. Intel√¢¬Ä¬ôs CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \n",
      "$20 billion to $100 billion. That would be one of the biggest investments in manufacturing in American history. And all they√¢¬Ä¬ôre waiting for is for you to pass this bill. So let√¢¬Ä¬ôs not wait any longer. Send it to my desk. I√¢¬Ä¬ôll sign it. And we will really take off. And Intel is not alone.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings, breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "docs = text_splitter.create_documents([api_text])\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n",
    "print(docs[1].page_content)\n",
    "print(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad865609-356b-49a0-b86f-9009907124ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
